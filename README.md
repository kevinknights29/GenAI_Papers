# GenAI_Papers

This project contains a list of interesting research papers in the field of GenAI.

## Topics

1. [Overview](#overview)
2. [Goals](#goals)
3. [Scope and Context](#scope-and-context)
4. [Research Papers](#research-papers)
5. [Learning Logs](#learning-logs)

---

## Overview

This repository is dedicated to the aggregation and discussion of groundbreaking research in the field of Generative AI.

Generative AI, or GenAI, refers to the subset of artificial intelligence focused on creating new content, ranging from text and images to code and beyond.
The collection of papers included herein spans a variety of topics within GenAI, such as Generative Adversarial Networks (GANs), Variational Autoencoders (VAEs), and Transformer-based models.

This compendium serves as a resource for scholars, practitioners, and enthusiasts seeking to advance the state of the art in AI-driven content generation.

## Goals

The primary goals of this repository are:

1. **Knowledge Consolidation**: To centralize seminal and cutting-edge research papers that define and advance the GenAI field.
2. **Community Collaboration**: To foster a collaborative environment where ideas and findings can be shared, discussed, and critiqued by the GenAI research community.
3. **Innovation Promotion**: To inspire and guide new research initiatives and practical applications of GenAI technologies.
4. **Interdisciplinary Integration**: To encourage the cross-pollination of ideas from diverse fields such as computer science, cognitive psychology, and digital arts to enrich the GenAI research.

## Scope and Context

### Scope

The scope of this repository is to encompass a wide array of research within GenAI, including but not limited to:

- Theoretical foundations of generative models
- Technical advancements in algorithm design
- Applications of GenAI in various domains (e.g., art, healthcare, software development)
- Ethical considerations and societal impacts of GenAI

### Context

The GenAI field is situated at the intersection of multiple disciplines. It leverages deep learning, statistical modeling, and computational creativity to generate novel outputs that can mimic or even surpass human-level creativity in certain aspects. With the rapid pace of advancement in AI, it is crucial to maintain a clear and organized overview of the progress in this area, which this repository aims to provide.

## Research Papers

NOTE: not in a particular order.

### Clustering

#### Language Models & General AI

Papers: 1, 10, 11, 13, 14, 15, 16, 17, 18, 19, 20, 21, 22, 23, 24, 31, 34, 35, 36, 37, 38.

These papers involve language models, their applications, ethical considerations, and various improvements in their training or functionality.

#### Vision & Language Integration

Papers: 2, 3, 4, 7, 29, 30, 33.

These focus on integrating visual data with language models, including vision transformers, image synthesis, and text-to-image personalization.

#### Attention Mechanisms & Transformers

Papers: 8, 9, 25, 28.

They discuss the theoretical aspects of attention in deep learning and methods to optimize transformer models.

#### Music & Creative AI

Paper: 5.

This paper seems unique in its focus on music generation using AI.

#### High-Resolution Image Synthesis

Papers: 6, 7.

These involve image synthesis with high resolution, discussing diffusion models and vision transformers.

#### Efficiency & Scaling in AI

Papers: 2, 25, 26, 27, 28.

They cover efficiency in AI, whether in terms of memory, inference, or the scaling of AI models.

#### Environmental Impact of AI

Paper: 12.

This paper is unique in its focus on the environmental impact of AI systems.

#### Dialog & Interaction-Focused AI

Papers: 13, 24, 34, 35, 36, 37.

These involve dialogue applications, interactive language models, and platforms for language agents.

#### AI Enhancement & Meta-Learning

Papers: 27, 31, 32, 37.

These papers are about improving AI's capabilities through self-improvement techniques, preference optimization, and distillation processes.

#### Miscellaneous AI Applications

Papers: 29, 30, 33.

These papers seem to discuss AI applications in more niche or specific contexts, like defeasible commonsense norms and visual instruction tuning.

### Complete List

1. [Enhancing CLIP with GPT-4: Harnessing Visual Descriptions as Prompts](https://browse.arxiv.org/pdf/2307.11661.pdf)
2. [EfficientViT: Memory Efficient Vision Transformer with Cascaded Group Attention](https://arxiv.org/pdf/2305.07027.pdf)
3. [Key-Locked Rank One Editing for Text-to-Image Personalization](https://browse.arxiv.org/pdf/2305.01644.pdf)
4. [ELIXR: Towards a general purpose X-ray artificial intelligence system through alignment of large language models and radiology vision encoders](https://arxiv.org/pdf/2308.01317.pdf)
5. [Simple and Controllable Music Generation](https://arxiv.org/pdf/2306.05284.pdf)
6. [High-Resolution Image Synthesis with Latent Diffusion Models](https://arxiv.org/pdf/2112.10752.pdf)
7. [All are Worth Words: A ViT Backbone for Diffusion Models](https://arxiv.org/pdf/2209.12152.pdf)
8. [Attention Is All You Need](https://arxiv.org/pdf/1706.03762.pdf)
9. [A Mathematical View of Attention Models in Deep Learning](https://people.tamu.edu/~sji/classes/attn.pdf)
10. [Improving Language Understanding by Generative Pre-Training](https://s3-us-west-2.amazonaws.com/openai-assets/research-covers/language-unsupervised/language_understanding_paper.pdf)
11. [Large Language Models and the Reverse Turing Test](https://www.ncbi.nlm.nih.gov/pmc/articles/PMC10177005/pdf/nihms-1876424.pdf)
12. [Estimating the Carbon Footprint of Bloom, a 176b Parameter Language Model](https://arxiv.org/pdf/2211.02001.pdf)
13. [LaMDA: Language Models for Dialog Applications](https://arxiv.org/pdf/2201.08239.pdf)
14. [Gorilla: Large Language Model Connected with Massive APIs](https://arxiv.org/pdf/2305.15334.pdf)
15. [Foundation Models for Decision Making Problems, Methods, and Opportunities](https://arxiv.org/pdf/2303.04129.pdf)
16. [Continual Pre-training of Language Models](https://arxiv.org/pdf/2302.03241.pdf)
17. [How Far Can Camels Go? Exploring the State of Instruction Tuning on Open Resources](https://arxiv.org/pdf/2306.04751.pdf)
18. [Alpagasus: Training a Better Alpaca with Fewer Data](https://arxiv.org/pdf/2307.08701.pdf)
19. [Ethical and social risks of harm from Language Models](https://arxiv.org/pdf/2112.04359.pdf)
20. [Holistic Evaluation of Language Models](https://arxiv.org/pdf/2211.09110.pdf)
21. [On the Risk of Misinformation Pollution with Large Language Models](https://arxiv.org/pdf/2305.13661.pdf)
22. [The Capacity for Moral Self-Correction in Large Language Models](https://arxiv.org/pdf/2302.07459.pdf)
23. [HONEST: Measuring Hurtful Sentence Completion in Language Models](https://aclanthology.org/2021.naacl-main.191.pdf)
24. [ReAct: Synergizing Reasoning and Acting in Language Models](https://arxiv.org/pdf/2210.03629.pdf)
25. [Efficiently Scaling Transformer Inference](https://arxiv.org/pdf/2211.05102.pdf)
26. [Hungry Hungry Hippos: Towards Language Modeling with State Space Models](https://arxiv.org/pdf/2212.14052.pdf)
27. [Promptbreeder: Self-Referential Self-Improvement via Prompt Evolution](https://arxiv.org/pdf/2309.16797.pdf)
28. [Efficient Streaming Language Models with Attention Sinks](https://arxiv.org/pdf/2309.17453v1.pdf)
29. [Visual Instruction Tuning](https://arxiv.org/pdf/2304.08485.pdf)
30. [Improved Baselines with Visual Instruction Tuning](https://arxiv.org/pdf/2310.03744.pdf)
31. [Direct Preference Optimization: Your Language Model is Secretly a Reward Model](https://arxiv.org/pdf/2305.18290.pdf)
32. [Distil-Whisper: Robust Knowledge Distilation via Large-Scale Pseudo Labelling](https://github.com/huggingface/distil-whisper/blob/main/Distil_Whisper.pdf)
33. [Reading Books is Great, But Not if You Are Driving! Visually Grounded Reasoning about Defeasible Commonsense Norms](https://arxiv.org/pdf/2310.10418.pdf)
34. [TeacherLM: Teaching to Fish Rather Than Giving the Fish, Language Modeling Likewise](https://arxiv.org/pdf/2310.19019.pdf)
35. [Self-RAG: Learning to Retrieve, Generate, and Critique through Self-Reflection](https://arxiv.org/pdf/2310.11511.pdf)
36. [InstructRetro: Instruction Tuning post Retrieval-Augmented Pretraining](https://arxiv.org/pdf/2310.07713.pdf)
37. [OpenAgents: An Open Platform for Language Agents in the Wild](https://arxiv.org/pdf/2310.10634.pdf)
38. [Large Language Models Understand and Can be Enhanced by Emotional Stimuli](https://arxiv.org/pdf/2307.11760.pdf)

## Learning Logs

| Date | Learning |
|------|----------|
|      |          |
