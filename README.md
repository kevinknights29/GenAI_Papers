# GenAI_Papers

This project contains a list of interesting research papers in the field of GenAI.

## Topics

1. [Overview](#overview)
2. [Goals](#goals)
3. [Scope and Context](#scope-and-context)
4. [Research Papers](#research-papers)
5. [Learning Logs](#learning-logs)

---

## Overview

This repository is dedicated to the aggregation and discussion of groundbreaking research in the field of Generative AI.

Generative AI, or GenAI, refers to the subset of artificial intelligence focused on creating new content, ranging from text and images to code and beyond.
The collection of papers included herein spans a variety of topics within GenAI, such as Generative Adversarial Networks (GANs), Variational Autoencoders (VAEs), and Transformer-based models.

This compendium serves as a resource for scholars, practitioners, and enthusiasts seeking to advance the state of the art in AI-driven content generation.

## Goals

The primary goals of this repository are:

1. **Knowledge Consolidation**: To centralize seminal and cutting-edge research papers that define and advance the GenAI field.
2. **Community Collaboration**: To foster a collaborative environment where ideas and findings can be shared, discussed, and critiqued by the Gen AI research community.
3. **Innovation Promotion**: To inspire and guide new research initiatives and practical applications of GenAI technologies.
4. **Interdisciplinary Integration**: To encourage the cross-pollination of ideas from diverse fields such as computer science, cognitive psychology, and digital arts to enrich the GenAI research.

## Scope and Context

### Scope

The scope of this repository is to encompass a wide array of research within GenAI, including but not limited to:

- Theoretical foundations of generative models
- Technical advancements in algorithm design
- Applications of GenAI in various domains (e.g., art, healthcare, software development)
- Ethical considerations and societal impacts of GenAI

### Context

The GenAI field is situated at the intersection of multiple disciplines. It leverages deep learning, statistical modeling, and computational creativity to generate novel outputs that can mimic or even surpass human-level creativity in certain aspects. With the rapid pace of advancement in AI, it is crucial to maintain a clear and organized overview of the progress in this area, which this repository aims to provide.

## Research Papers

> :memo: **Note:** Not in a particular order.

### Classification

| Category | Papers | Description |
|----------|--------|-------------|
| Language Models & General AI | 1, 10, 11, 13, 14, 15, 16, 17, 18, 19, 20, 21, 22, 23, 24, 31, 34, 35, 36, 37, 38, 40, 41, 42, 43, 44, 45, 48, 54, 56, 58, 60, 66, 69, 74, 76, 79, 80, 82, 84, 86, 87, 89, 90, 92, 93, 95, 98, 99, 101, 103, 104 | Papers related to language models, their applications, ethical considerations, and improvements in training or functionality. |
| Vision & Language Integration | 3, 4, 29, 30, 33, 64  | Focusing on the integration of visual data with language models, including vision transformers and text-to-image personalization. |
| Attention Mechanisms & Transformers | 8, 9, 25, 28, 73 | Discussing the theory of attention in deep learning and optimization of transformer models. |
| Music & Creative AI | 5 | A unique paper on music generation using AI. |
| High-Resolution Image Synthesis | 6, 7, 63 | Discussing high-resolution image synthesis using diffusion models and vision transformers. |
| Efficiency & Scaling in AI | 2, 25, 26, 27, 28, 59, 61, 71, 72, 83, 88, 97 | Covering AI efficiency in terms of memory, inference, and scaling. |
| Environmental Impact of AI | 12 | A unique paper focusing on the environmental impact of AI systems. |
| Dialog & Interaction-Focused AI | 13, 24, 34, 35, 36, 37, 39, 53, 67, 81, 91 | Involving dialogue applications and platforms for interactive language agents. |
| AI Enhancement & Meta-Learning | 27, 31, 32, 37, 46, 47, 49, 55, 57, 62, 65, 68, 70, 75, 78, 96 | On improving AI capabilities through self-improvement, preference optimization, and distillation. |
| Miscellaneous AI Applications | 29, 30, 33, 50, 52, 77, 85, 94, 100, 102 | Discussing niche AI applications like commonsense norms and visual instruction tuning. |

### Complete List

1. [Enhancing CLIP with GPT-4: Harnessing Visual Descriptions as Prompts](https://browse.arxiv.org/pdf/2307.11661.pdf)
2. [EfficientViT: Memory Efficient Vision Transformer with Cascaded Group Attention](https://arxiv.org/pdf/2305.07027.pdf)
3. [Key-Locked Rank One Editing for Text-to-Image Personalization](https://browse.arxiv.org/pdf/2305.01644.pdf)
4. [ELIXR: Towards a general purpose X-ray artificial intelligence system through alignment of large language models and radiology vision encoders](https://arxiv.org/pdf/2308.01317.pdf)
5. [Simple and Controllable Music Generation](https://arxiv.org/pdf/2306.05284.pdf)
6. [High-Resolution Image Synthesis with Latent Diffusion Models](https://arxiv.org/pdf/2112.10752.pdf)
7. [All are Worth Words: A ViT Backbone for Diffusion Models](https://arxiv.org/pdf/2209.12152.pdf)
8. [Attention Is All You Need](https://arxiv.org/pdf/1706.03762.pdf)
9. [A Mathematical View of Attention Models in Deep Learning](https://people.tamu.edu/~sji/classes/attn.pdf)
10. [Improving Language Understanding by Generative Pre-Training](https://s3-us-west-2.amazonaws.com/openai-assets/research-covers/language-unsupervised/language_understanding_paper.pdf)
11. [Large Language Models and the Reverse Turing Test](https://www.ncbi.nlm.nih.gov/pmc/articles/PMC10177005/pdf/nihms-1876424.pdf)
12. [Estimating the Carbon Footprint of Bloom, a 176b Parameter Language Model](https://arxiv.org/pdf/2211.02001.pdf)
13. [LaMDA: Language Models for Dialog Applications](https://arxiv.org/pdf/2201.08239.pdf)
14. [Gorilla: Large Language Model Connected with Massive APIs](https://arxiv.org/pdf/2305.15334.pdf)
15. [Foundation Models for Decision Making Problems, Methods, and Opportunities](https://arxiv.org/pdf/2303.04129.pdf)
16. [Continual Pre-training of Language Models](https://arxiv.org/pdf/2302.03241.pdf)
17. [How Far Can Camels Go? Exploring the State of Instruction Tuning on Open Resources](https://arxiv.org/pdf/2306.04751.pdf)
18. [Alpagasus: Training a Better Alpaca with Fewer Data](https://arxiv.org/pdf/2307.08701.pdf)
19. [Ethical and social risks of harm from Language Models](https://arxiv.org/pdf/2112.04359.pdf)
20. [Holistic Evaluation of Language Models](https://arxiv.org/pdf/2211.09110.pdf)
21. [On the Risk of Misinformation Pollution with Large Language Models](https://arxiv.org/pdf/2305.13661.pdf)
22. [The Capacity for Moral Self-Correction in Large Language Models](https://arxiv.org/pdf/2302.07459.pdf)
23. [HONEST: Measuring Hurtful Sentence Completion in Language Models](https://aclanthology.org/2021.naacl-main.191.pdf)
24. [ReAct: Synergizing Reasoning and Acting in Language Models](https://arxiv.org/pdf/2210.03629.pdf)
25. [Efficiently Scaling Transformer Inference](https://arxiv.org/pdf/2211.05102.pdf)
26. [Hungry Hungry Hippos: Towards Language Modeling with State Space Models](https://arxiv.org/pdf/2212.14052.pdf)
27. [Promptbreeder: Self-Referential Self-Improvement via Prompt Evolution](https://arxiv.org/pdf/2309.16797.pdf)
28. [Efficient Streaming Language Models with Attention Sinks](https://arxiv.org/pdf/2309.17453v1.pdf)
29. [Visual Instruction Tuning](https://arxiv.org/pdf/2304.08485.pdf)
30. [Improved Baselines with Visual Instruction Tuning](https://arxiv.org/pdf/2310.03744.pdf)
31. [Direct Preference Optimization: Your Language Model is Secretly a Reward Model](https://arxiv.org/pdf/2305.18290.pdf)
32. [Distil-Whisper: Robust Knowledge Distilation via Large-Scale Pseudo Labelling](https://github.com/huggingface/distil-whisper/blob/main/Distil_Whisper.pdf)
33. [Reading Books is Great, But Not if You Are Driving! Visually Grounded Reasoning about Defeasible Commonsense Norms](https://arxiv.org/pdf/2310.10418.pdf)
34. [TeacherLM: Teaching to Fish Rather Than Giving the Fish, Language Modeling Likewise](https://arxiv.org/pdf/2310.19019.pdf)
35. [Self-RAG: Learning to Retrieve, Generate, and Critique through Self-Reflection](https://arxiv.org/pdf/2310.11511.pdf)
36. [InstructRetro: Instruction Tuning post Retrieval-Augmented Pretraining](https://arxiv.org/pdf/2310.07713.pdf)
37. [OpenAgents: An Open Platform for Language Agents in the Wild](https://arxiv.org/pdf/2310.10634.pdf)
38. [Large Language Models Understand and Can be Enhanced by Emotional Stimuli](https://arxiv.org/pdf/2307.11760.pdf)
39. [Communicative Agents for Software Development](https://arxiv.org/pdf/2307.07924v3.pdf)
40. [Large Language Models Are Human-Level Prompt Engineers](https://arxiv.org/pdf/2211.01910.pdf)
41. [Least-to-Most Prompting Enables Complex Reasoning in Large Language Models](https://arxiv.org/abs/2205.10625)
42. [Self-Consistency Improves Chain of Thought Reasoning in Language Models](https://arxiv.org/abs/2203.11171.pdf)
43. [Language Models can be Logical Solvers](https://arxiv.org/pdf/2311.06158.pdf)
44. [Lost in the Middle: How Language Models Use Long Contexts](https://arxiv.org/pdf/2307.03172.pdf)
45. [Contrastive Chain-of-Thought Prompting](https://arxiv.org/pdf/2311.09277.pdf)
46. [RankZephyr: Effective and Robust Zero-Shot Listwise Reranking is a Breeze!](https://arxiv.org/pdf/2312.02724.pdf)
47. [LLM in a flash: Efficient Large Language Model Inference with Limited Memory](https://arxiv.org/pdf/2312.11514.pdf)
48. [PowerInfer: Fast Large Language Model Serving with a Consumer-grade GPU](https://ipads.se.sjtu.edu.cn/_media/publications/powerinfer-20231219.pdf)
49. [Human Centered Loss Functions (HALOs)](https://github.com/ContextualAI/HALOs/blob/main/assets/report.pdf)
50. [A Challenger to GPT-4V? Early Explorations of Gemini in Visual Expertise](https://arxiv.org/pdf/2312.12436.pdf)
51. [Distributed Inference and Fine-tuning of Large Language Models Over The Internet](https://arxiv.org/pdf/2312.08361.pdf)
52. [GAIA: Zero-shot Talking Avatar Generation](https://arxiv.org/pdf/2311.15230.pdf)
53. [SLEEPER AGENTS: TRAINING DECEPTIVE LLMS THAT PERSIST THROUGH SAFETY TRAINING](https://arxiv.org/pdf/2401.05566.pdf)
54. [LLMLingua: Compressing Prompts for Accelerated Inference of Large Language Models](https://arxiv.org/pdf/2310.05736.pdf)
55. [Foundations of Vector Retrieval](https://arxiv.org/pdf/2401.09350.pdf)
56. [Self-Rewarding Language Models](https://arxiv.org/pdf/2401.10020.pdf)
57. [BloombergGPT: A Large Language Model for Finance](https://arxiv.org/pdf/2303.17564.pdf)
58. [Mistral 7B](https://arxiv.org/pdf/2310.06825.pdf)
59. [Outrageously Large Neural Networks: The Sparsely-Gated Mixture-of-Experts Layer](https://arxiv.org/pdf/1701.06538.pdf)
60. [MegaBlocks: Efficient Sparse Training with Mixture-of-Experts](https://arxiv.org/pdf/2211.15841.pdf)
61. [Mixture-of-Experts Meets Instruction Tuning: A Winning Combination for Large Language Models](https://arxiv.org/pdf/2305.14705.pdf)
62. [Orca 2: Teaching Small Language Models How to Reason](https://arxiv.org/pdf/2311.11045.pdf)
63. [ConvNets Match Vision Transformers at Scale](https://arxiv.org/pdf/2310.16764.pdf)
64. [Emu Video: Factorizing Text-to-Video Generation by Explicit Image Conditioning](https://arxiv.org/pdf/2311.10709.pdf)
65. [Pythia: A Suite for Analyzing Large Language Models Across Training and Scaling](https://arxiv.org/pdf/2304.01373.pdf)
66. [Llama 2: Open Foundation and Fine-Tuned Chat Models](https://arxiv.org/pdf/2307.09288.pdf)
67. [QLoRA: Efficient Finetuning of Quantized LLMs](https://arxiv.org/pdf/2305.14314.pdf)
68. [RAG vs Fine-tuning: Pipelines, Tradeoffs, and a Case Study on Agriculture](https://arxiv.org/pdf/2401.08406.pdf)
69. [Training language models to follow instructions with human feedback](https://arxiv.org/pdf/2203.02155.pdf)
70. [Spotting LLMs With Binoculars: Zero-Shot Detection of Machine-Generated Text](https://arxiv.org/pdf/2401.12070.pdf)
71. [Sparse Networks from Scratch: Faster Training without Losing Performance](https://arxiv.org/pdf/1907.04840.pdf)
72. [ZeRO: Memory Optimizations Toward Training Trillion Parameter Models](https://arxiv.org/pdf/1910.02054.pdf)
73. [Switch Transformers: Scaling to Trillion Parameter Models with Simple and Efficient Sparsity](https://arxiv.org/pdf/2101.03961.pdf)
74. [DeepSeekMoE: Towards Ultimate Expert Specialization in Mixture-of-Experts Language Models](https://arxiv.org/pdf/2401.06066.pdf)
75. [MoE-Mamba: Efficient Selective State Space Models with Mixture of Experts](https://arxiv.org/pdf/2401.04081.pdf)
76. [Code Llama: Open Foundation Models for Code](https://scontent.fpac4-1.fna.fbcdn.net/v/t39.2365-6/369856151_1754812304950972_1159666448927483931_n.pdf?_nc_cat=107&ccb=1-7&_nc_sid=3c67a6&_nc_ohc=XDhDRw2RiP8AX_JzqMR&_nc_ht=scontent.fpac4-1.fna&oh=00_AfB003nd4Kls4aOSwchgL5F3cvbIRRkGF5VxjgQOnn_WWw&oe=65BE508F)
77. [LLaMA Pro: Progressive LLaMA with Block Expansion](https://arxiv.org/pdf/2401.02415.pdf)
78. [Self-Play Fine-Tuning Converts Weak Language Models to Strong Language Models](https://arxiv.org/pdf/2401.01335.pdf)
79. [Monarch Mixer: A Simple Sub-Quadratic GEMM-Based Architecture](https://arxiv.org/pdf/2310.12109.pdf)
80. [Large Language Model based Multi-Agents: A Survey of Progress and Challenges](https://arxiv.org/pdf/2402.01680.pdf)
81. [Retrieval-Augmented Generation for Large Language Models: A Survey](https://arxiv.org/pdf/2312.10997.pdf)
82. [ReAugKD: Retrieval-Augmented Knowledge Distillation For Pre-trained Language Models](https://assets.amazon.science/31/1d/ce1589b74f228a19c32c73123f49/reaugkd-retrieval-augmented-knowledge-distillation-for-pre-trained-language-models.pdf)
83. [The Era of 1-bit LLMs: All Large Language Models are in 1.58 Bits](https://arxiv.org/pdf/2402.17764.pdf)
84. [Assisting in Writing Wikipedia-like Articles From Scratch with Large Language Models](https://arxiv.org/pdf/2402.14207.pdf)
85. [RAPTOR: Recursive Abstractive Processing for Tree-Organized Retrieval](https://arxiv.org/pdf/2401.18059v1.pdf)
86. [Beyond A*: Better Planning with Transformers via Search Dynamics Bootstrapping](https://arxiv.org/abs/2402.14083)
87. [Datasets for Large Language Models: A Comprehensive Survey](https://arxiv.org/pdf/2402.18041.pdf)
88. [An LLM Compiler for Parallel Function Calling](https://arxiv.org/pdf/2312.04511.pdf)
89. [Chain-of-Thought Prompting Elicits Reasoning in Large Language Models](https://arxiv.org/pdf/2201.11903.pdf)
90. [Plan-and-Solve Prompting: Improving Zero-Shot Chain-of-Thought Reasoning by Large Language Models](https://arxiv.org/pdf/2305.04091.pdf)
91. [ReWOO: Decoupling Reasoning from Observations for Efficient Augmented Language Models](https://arxiv.org/pdf/2305.18323.pdf)
92. [StructLM: Towards Building Generalist Models for Structured Knowledge Grounding](https://arxiv.org/pdf/2402.16671.pdf)
93. [A Critical Evaluation of AI Feedback for Aligning Large Language Models](https://arxiv.org/pdf/2402.12366.pdf)
94. [Are More LLM Calls All You Need? Towards Scaling Laws of Compound Inference Systems](https://arxiv.org/pdf/2403.02419.pdf)
95. [Are Emergent Abilities of Large Language Models a Mirage?](https://arxiv.org/pdf/2304.15004.pdf)
96. [Yi: Open Foundation Models by 01.AI](https://arxiv.org/pdf/2403.04652.pdf)
97. [ORPO: Monolithic Preference Optimization without Reference Model](https://arxiv.org/pdf/2403.07691.pdf)
98. [Do Large Language Models Understand Logic or Just Mimick Context?](https://arxiv.org/pdf/2402.12091.pdf)
99. [Evaluating Large Language Models Trained on Code](https://arxiv.org/pdf/2107.03374.pdf)
100. [Self-Refine: Iterative Refinement with Self-Feedback](https://openreview.net/pdf?id=S37hOerQLB)
101. [Reflexion: Language Agents with Verbal Reinforcement Learning](https://arxiv.org/pdf/2303.11366.pdf)
102. [MM-REACT: Prompting ChatGPT for Multimodal Reasoning and Action](https://arxiv.org/pdf/2303.11381.pdf)
103. [HuggingGPT: Solving AI Tasks with ChatGPT and its Friends in Hugging Face](https://arxiv.org/pdf/2303.17580.pdf)
104. [AutoGen: Enabling Next-Gen LLM Applications via Multi-Agent Conversation](https://arxiv.org/pdf/2308.08155.pdf)
105. [Megalodon: Efficient LLM Pretraining and Inference with Unlimited Context Length](https://arxiv.org/pdf/2404.08801.pdf)
106. [A survey of Generative AI Applications](https://arxiv.org/pdf/2306.02781)
107. [MAC-SQL: A Multi-Agent Collaborative Framework for Text-to-SQL](https://arxiv.org/pdf/2312.11242)
108. [Prometheus 2: An Open Source Language Model Specialized in Evaluating Other Language Models](https://arxiv.org/pdf/2405.01535)
109. [MetaGPT: Meta Programming For A Multi-Agent Collaborative Framework](https://arxiv.org/pdf/2308.00352)
110. [Understanding Transformer Reasoning Capabilities via Graph Algorithms](https://arxiv.org/pdf/2405.18512)
111. [Banishing LLM Hallucinations Requires Rethinking Generalization](https://arxiv.org/pdf/2406.17642)
112. [Your Context Is Not an Array: Unveiling Random Access Limitations in Transformers](https://arxiv.org/pdf/2408.05506)
113. [LLM Lies: Hallucinations are not Bugs, but Features as Adversarial Examples](https://arxiv.org/pdf/2310.01469)
114. [Memory^3 : Language Modeling with Explicit Memory](https://arxiv.org/pdf/2407.01178)
115. [NeuroLogic Decoding: (Un)supervised Neural Text Generation with Predicate Logic Constraints](https://aclanthology.org/2021.naacl-main.339.pdf)
116. [LOTUS: Enabling Semantic Queries with LLMs Over Tables of Unstructured and Structured Data](https://arxiv.org/pdf/2407.11418)
117. [Text2SQL is Not Enough: Unifying AI and Databases with TAG](https://arxiv.org/pdf/2408.14717)

## Learning Logs

| Date | Learning |
|------|----------|
|      |          |
